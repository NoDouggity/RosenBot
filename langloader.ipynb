{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import keys\n",
    "import cohere\n",
    "import math\n",
    "import pymongo\n",
    "import requests\n",
    "import psycopg2\n",
    "\n",
    "def batch_embed(client, objects, batch_size=96):\n",
    "    total_objects = len(objects)\n",
    "    num_batches = math.ceil(total_objects / batch_size)\n",
    "    embeds = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i+1) * batch_size, total_objects)\n",
    "        batch = objects[start_index:end_index]\n",
    "\n",
    "        response = client.embed(\n",
    "            texts=batch, model=\"embed-english-v3.0\", input_type = \"search_document\"\n",
    "        )\n",
    "        embeds+=response.embeddings\n",
    "    \n",
    "    return embeds\n",
    "        \n",
    "def mongo_to_df(results):\n",
    "    list = []\n",
    "    for row in results:\n",
    "        list.append(row)\n",
    "    return pd.DataFrame(list)\n",
    "\n",
    "pgconn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname='caroldb',\n",
    "    user='postgres',\n",
    "    password=keys.pg_pwd\n",
    ")\n",
    "co = cohere.Client(keys.cohere_key)\n",
    "pgcur = pgconn.cursor()\n",
    "\n",
    "# get all links to transcripts\n",
    "pgcur.execute(\"\"\"SELECT hyperlink FROM linktable WHERE hyperlink LIKE '%TRANS%'\"\"\")\n",
    "linktable = pd.DataFrame(pgcur.fetchall(), columns=['hyperlink'])\n",
    "pgcur.execute(\"\"\"SELECT hyperlink FROM transcripts\"\"\")\n",
    "trans_completed = pd.DataFrame(pgcur.fetchall(), columns=['hyperlink'])\n",
    "\n",
    "# select only uncompleted transcripts\n",
    "dedup = pd.concat([linktable,trans_completed]).drop_duplicates(subset='hyperlink',keep=False)\n",
    "\n",
    "batch_size = 10\n",
    "num_batches = math.ceil(len(dedup) / batch_size)\n",
    "print(f'Total links to process: {len(dedup)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(num_batches):\n",
    "    print(f'Begin processing batch {i+1}')\n",
    "    batch_df = dedup[i*batch_size : min((i+1)*batch_size,len(dedup))]\n",
    "    urls = batch_df['hyperlink'].to_list()\n",
    "    summaries = []\n",
    "\n",
    "    for url in urls:\n",
    "        document_name = url[(url.find('KSM2/'))+5:]\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(document_name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"file {url} downloaded successfully\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Error downloading file {url}: {e}')\n",
    "        loader = PyPDFLoader(document_name)\n",
    "        pages = loader.load_and_split()\n",
    "        \n",
    "        if len(pages) > 0:\n",
    "            # clean the pages\n",
    "            full_text = ''\n",
    "            for page in pages:\n",
    "                lines = page.page_content.split('\\n')\n",
    "                lines_trimmed = list(map(str.strip, lines))\n",
    "                cleaned_text = list(map(lambda s: re.sub(r'\\s+\\d{1,2}\\s+$', '', s), lines))[3:-2]\n",
    "                page.page_content = '\\n'.join(cleaned_text)\n",
    "                full_text+=(page.page_content+'\\n')\n",
    "            \n",
    "            # create a Series from the pages\n",
    "            spages = pd.Series([page.page_content for page in pages])\n",
    "            \n",
    "            # get embeddings\n",
    "            print(f'fetching embeddings for {document_name}')\n",
    "            embeds = pd.Series(batch_embed(co,spages.to_list(),96))\n",
    "\n",
    "            # add page numbers\n",
    "            page_numbers = pd.Series(range(1,len(pages)+1))\n",
    "            \n",
    "            # concat into dataframe and add parent doc name\n",
    "            df = pd.DataFrame({'page_number':page_numbers, 'text':spages, 'embedding':embeds})\n",
    "            df.insert(0, 'parent_doc', document_name)\n",
    "\n",
    "            # insert pages into database\n",
    "            page_records = list(df.itertuples(index=False,name=None))\n",
    "            query = \"\"\" \n",
    "                INSERT INTO pages (parent_doc, page_number, text, embedding)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            pgcur.executemany(query, page_records)\n",
    "            \n",
    "            # generate summary of full_text and add to summaries list\n",
    "            print(f'generating summary for {document_name}')\n",
    "            summary = co.chat(\n",
    "                preamble=keys.preamble,\n",
    "                message=full_text,\n",
    "                max_tokens=500,\n",
    "                temperature=.2\n",
    "            )\n",
    "            try:\n",
    "                summaries.append(summary.text)\n",
    "            except:\n",
    "                summaries.append('No summary available')\n",
    "        else:\n",
    "            summaries.append('No summary available')    \n",
    "            \n",
    "        os.remove(document_name)\n",
    "\n",
    "    batch_df['Summaries'] = summaries\n",
    "    transcript_records = list(batch_df.itertuples(index=False,name=None))\n",
    "    query = \"\"\" \n",
    "        INSERT INTO transcripts (record_id, filingname, filingdate, aenumber, attachment, hyperlink, summary)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    pgcur.executemany(query, transcript_records)\n",
    "    pgconn.commit()\n",
    "\n",
    "pgconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import keys\n",
    "import pandas as pd\n",
    "\n",
    "mongo = pymongo.MongoClient(keys.mongodb_cs)\n",
    "\n",
    "linktable = mongo['Transcripts']['linktable'].aggregate([{\n",
    "    '$search': {\n",
    "        'index': 'linktable_index',\n",
    "        'regex': {\n",
    "            'path': 'Hyperlink',\n",
    "            'query': '(.*)TRANS(.*)',\n",
    "            \"allowAnalyzedField\": True\n",
    "        },}}])\n",
    "linktable_df = mongo_to_df(linktable)\n",
    "\n",
    "mongo.Transcripts.transcripts.aggregate([\n",
    "    {\"$match\": {}},\n",
    "    {\"$group\": { \"_id\": { \"Hyperlink\": \"$Hyperlink\" },\n",
    "        \"doc\": { \"$last\": \"$$ROOT\" }}}, # Retrieve only last doc in a group\n",
    "    {\"$replaceRoot\": { \"newRoot\": \"$doc\" }}, # replace doc as object as new root of document,\n",
    "    { \"$out\" : 'transcripts' } # Test above aggregation & then use this \n",
    "  ])\n",
    "\n",
    "mongo.Transcripts.pages_copy.drop()\n",
    "parent_doc_list = linktable_df['Hyperlink'].to_list()\n",
    "for parent_doc in parent_doc_list:\n",
    "  doc_name = parent_doc[(parent_doc.find('KSM2/'))+5:]\n",
    "  mongo.Transcripts.pages.aggregate([\n",
    "    {\n",
    "      \"$match\": { \"parent_doc\" : doc_name}\n",
    "    },\n",
    "    {\n",
    "      \"$group\": { \"_id\": { \"page_number\": \"$page_number\" },\n",
    "        \"doc\": { \"$last\": \"$$ROOT\" } # Retrieve only last doc in a group\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"$replaceRoot\": { \"newRoot\": \"$doc\" } # replace doc as object as new root of document\n",
    "    },\n",
    "    { \"$merge\" : 'pages_copy' } # Test above aggregation & then use this \n",
    "  ])\n",
    "  mongo.Transcripts.pages.delete_many({\"parent_doc\":doc_name})\n",
    "\n",
    "mongo.Transcripts.pages.drop()\n",
    "mongo.Transcripts.pages_copy.rename(new_name='pages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
