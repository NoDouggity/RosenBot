{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import config\n",
    "import cohere\n",
    "import math\n",
    "import pymongo\n",
    "import requests\n",
    "\n",
    "def batch_embed(client, objects, batch_size=96):\n",
    "    total_objects = len(objects)\n",
    "    num_batches = math.ceil(total_objects / batch_size)\n",
    "    embeds = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i+1) * batch_size, total_objects)\n",
    "        batch = objects[start_index:end_index]\n",
    "\n",
    "        response = client.embed(\n",
    "            texts=batch, model=\"embed-english-v3.0\", input_type = \"search_document\"\n",
    "        )\n",
    "        embeds+=response.embeddings\n",
    "    \n",
    "    return embeds\n",
    "        \n",
    "mongo = pymongo.MongoClient(config.mongodb_cs)\n",
    "co = cohere.Client(config.cohere_key)\n",
    "\n",
    "linktable = mongo['Transcripts']['linktable'].aggregate([\n",
    "    {\n",
    "        '$search': {\n",
    "            'index': 'linktable_index',\n",
    "            'regex': {\n",
    "                'path': 'Hyperlink',\n",
    "                'query': '(.*)TRANS(.*)',\n",
    "                \"allowAnalyzedField\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    ")\n",
    "\n",
    "linktable_list = []\n",
    "for row in linktable:\n",
    "    linktable_list.append(row)\n",
    "linktable_df = pd.DataFrame(linktable_list)\n",
    "urls = linktable_df['Hyperlink'].to_list()\n",
    "summaries = []\n",
    "\n",
    "for url in urls[0:3]:\n",
    "    \n",
    "    document_name = url[(url.find('KSM2/'))+5:]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(document_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"file {url} downloaded successfully\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error downloading file {url}: {e}')\n",
    "    loader = PyPDFLoader(document_name)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    # clean the pages\n",
    "    full_text = ''\n",
    "    for page in pages:\n",
    "        lines = page.page_content.split('\\n')\n",
    "        lines_trimmed = list(map(str.strip, lines))\n",
    "        cleaned_text = list(map(lambda s: re.sub(r'\\s+\\d{1,2}\\s+$', '', s), lines))[3:-2]\n",
    "        page.page_content = '\\n'.join(cleaned_text)\n",
    "        full_text+=(page.page_content+'\\n')\n",
    "    \n",
    "    # create a Series from the pages\n",
    "    spages = pd.Series([page.page_content for page in pages])\n",
    "    \n",
    "    # get embeddings\n",
    "    embeds = pd.Series(batch_embed(co,spages.to_list(),96))\n",
    "\n",
    "    # add page numbers\n",
    "    page_numbers = pd.Series(range(1,len(pages)+1))\n",
    "    \n",
    "    # concat into dataframe and add parent doc name\n",
    "    df = pd.DataFrame({'page_number':page_numbers, 'text':spages, 'embeddings':embeds})\n",
    "    df.insert(0, 'parent_doc', document_name)\n",
    "\n",
    "    # insert pages into database\n",
    "    mongo.Transcripts.pages.insert_many(df.to_dict(orient='records'))\n",
    "    \n",
    "    # generate summary of full_text and add to summaries list\n",
    "    preamble = 'You are Carol RosenBot, a helpful AI language model. The following text is a transcript of a hearing in the military commission trial of several men accused of responsibility for plotting the 9/11 terror attacks. The Accused in this case are Khalid Sheikh Mohammed, Khallad Bin Attash, Mustafa al-Hawsawi, Ammar al-Baluchi, and Ramzi bin al-Shibh. Your task is to generate a summary of the below transcript. Your summary should be of moderate length and begin with \"In this hearing\". Try to sprinkle in some details.\\n\\n'\n",
    "    summary = co.chat(\n",
    "        preamble=preamble,\n",
    "        message=full_text\n",
    "    )\n",
    "    try:\n",
    "        summaries.append(summary.text)\n",
    "    except:\n",
    "        summaries.append('No summary available')\n",
    "    \n",
    "    os.remove(document_name)\n",
    "linktable_df_test = linktable_df[0:3]\n",
    "linktable_df_test['Summaries'] = summaries\n",
    "mongo.Transcripts.transcripts.insert_many(linktable_df_test.to_dict(orient='records'))\n",
    "\n",
    "mongo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagechunker(chunk_size, overlap_size, pages): # takes a list of Documents (pages)\n",
    "    start_page = 0\n",
    "    total_pages = len(pages)\n",
    "    chunks = []\n",
    "    while start_page < total_pages:\n",
    "        chunk_text = ''\n",
    "        for page in pages[start_page:min(start_page+chunk_size, total_pages-1)]:\n",
    "            chunk_text+=(page.page_content)\n",
    "        start_page = start_page+overlap_size\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "chunks = pagechunker(40, 10, pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
