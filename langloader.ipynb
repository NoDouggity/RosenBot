{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import config\n",
    "import cohere\n",
    "import math\n",
    "import pymongo\n",
    "\n",
    "def batch_embed(client, objects, batch_size=96):\n",
    "    total_objects = len(objects)\n",
    "    num_batches = math.ceil(total_objects / batch_size)\n",
    "    embeds = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i+1) * batch_size, total_objects)\n",
    "        batch = objects[start_index:end_index]\n",
    "\n",
    "        response = client.embed(\n",
    "            texts=batch, model=\"embed-english-v3.0\", input_type = \"search_document\"\n",
    "        )\n",
    "        embeds+=response.embeddings\n",
    "    \n",
    "    return embeds\n",
    "        \n",
    "\n",
    "files = os.listdir('transcripts/') # replace with GET of MDB linktree\n",
    "co = cohere.Client(config.cohere_key)\n",
    "\n",
    "for file in files[0:1]:\n",
    "    loader = PyPDFLoader('transcripts/'+file)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    # clean the pages\n",
    "    for page in pages:\n",
    "        lines = page.page_content.split('\\n')\n",
    "        lines_trimmed = list(map(str.strip, lines))\n",
    "        cleaned_text = list(map(lambda s: re.sub(r'\\s+\\d{1,2}\\s+$', '', s), lines))[3:-2]\n",
    "        page.page_content = '\\n'.join(cleaned_text)\n",
    "    \n",
    "    # create a Series from the pages\n",
    "    spages = pd.Series([page.page_content for page in pages])\n",
    "    \n",
    "    # get embeddings\n",
    "    embeds = pd.Series(batch_embed(co,spages.to_list(),96))\n",
    "\n",
    "    # add page numbers\n",
    "    page_numbers = pd.Series(range(1,len(pages)+1))\n",
    "    \n",
    "    # concat into dataframe and add parent doc name\n",
    "    df = pd.DataFrame({'page_number':page_numbers, 'text':spages, 'embeddings':embeds})\n",
    "    df.insert(0, 'parent_doc', file[0:-4])\n",
    "\n",
    "    # insert pages into database\n",
    "    mongo = pymongo.MongoClient(config.mongodb_cs)\n",
    "    mongo.Transcripts.pages.insert_many(df.to_dict(orient='records'))\n",
    "    \n",
    "    # with open('transcripts/'+file[:-3]+'txt', 'w') as f:\n",
    "    #         for page in pages:\n",
    "    #             f.write(page.page_content)\n",
    "    #             f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagechunker(chunk_size, overlap_size, pages): # takes a list of Documents (pages)\n",
    "    start_page = 0\n",
    "    total_pages = len(pages)\n",
    "    chunks = []\n",
    "    while start_page < total_pages:\n",
    "        chunk_text = ''\n",
    "        for page in pages[start_page:min(start_page+chunk_size, total_pages-1)]:\n",
    "            chunk_text+=(page.page_content)\n",
    "        start_page = start_page+overlap_size\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "chunks = pagechunker(40, 10, pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
